{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d98e32fb",
   "metadata": {},
   "source": [
    "Table of contents:\n",
    "\n",
    "1. [Imports](#Import)\n",
    "2. [Function definitions](#Functions)\n",
    "3. [Create JSON file](#JSON)\n",
    "4. [Prepare dataset](#Prep)\n",
    "5. [Define neural net](#NN)\n",
    "6. [Training](#Train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a3e50a",
   "metadata": {},
   "source": [
    "<a name = \"Import\"></a>\n",
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8a781bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\charl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "nltk.download('punkt')\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e69f76",
   "metadata": {},
   "source": [
    "<a name = \"Functions\"></a>\n",
    "## 2. Function definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7918bb22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sentence):\n",
    "    return nltk.word_tokenize(sentence)\n",
    "\n",
    "def stem(word):\n",
    "    stemmer = PorterStemmer()\n",
    "    return stemmer.stem(word=word.lower())\n",
    "\n",
    "def bag_of_words(tokenized_sentence, all_words):\n",
    "    tokenized_sentence = [stem(w) for w in tokenized_sentence]\n",
    "    \n",
    "    bag = np.zeros(len(all_words), dtype=np.float32)\n",
    "    for idx, w in enumerate(all_words):\n",
    "        if w in tokenized_sentence:\n",
    "            bag[idx] = 1.0\n",
    "            \n",
    "    return bag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8ba601",
   "metadata": {},
   "source": [
    "<a name = \"JSON\"></a>\n",
    "## 3. Create JSON file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06ff37b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "intents_key = pd.read_csv('intents-key.csv')\n",
    "intents_df = pd.read_csv('intents.csv')\n",
    "df = pd.read_csv('dataset_appended.csv')\n",
    "df = df.rename(columns={'Unnamed: 0': 'id', 'intent': 'tag', 'prompt': 'patterns', 'completion': 'responses'})\n",
    "\n",
    "intents = {'intents': []}\n",
    "tag_list = list(set(df['tag']))\n",
    "for i, tag in enumerate(tag_list):\n",
    "    intents['intents'].append(dict())\n",
    "    intents['intents'][i]['tag'] = tag\n",
    "    intents['intents'][i]['patterns'] = list(df[df['tag']==tag]['patterns'])\n",
    "    intents['intents'][i]['responses'] = list(set(intents_df[intents_df['intent']==tag]['completion']))\n",
    "    intents['intents'][i]['area'] = list(intents_df[intents_df['intent']==tag]['area'])[0]\n",
    "    intents['intents'][i]['context'] = intents_key[intents_key['area']==intents['intents'][i]['area']]['context'].iloc[0]\n",
    "    \n",
    "with open('intents.json', 'w') as f:\n",
    "    json.dump(intents, f) \n",
    "    \n",
    "with open('intents.json', 'r') as f:\n",
    "    intents = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7624baf",
   "metadata": {},
   "source": [
    "<a name = \"Prep\"></a>\n",
    "## 4. Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed2a17e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = []\n",
    "tags = []\n",
    "Xy = []\n",
    "\n",
    "for intent in intents['intents']:\n",
    "    tag = intent['tag']\n",
    "    tags.append(tag)\n",
    "    for pattern in intent['patterns']:\n",
    "        words = tokenize(pattern)\n",
    "        all_words.extend(words)\n",
    "        Xy.append((words, tag))    \n",
    "        \n",
    "ignore_words = ['?', '!', '.', ',']\n",
    "all_words = [stem(w) for w in all_words if w not in ignore_words]\n",
    "all_words = sorted(set(all_words))\n",
    "tags = sorted(set(tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9782c75d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4911, 559)\n",
      "(4911,)\n"
     ]
    }
   ],
   "source": [
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "for pattern_sentence, tag in Xy:\n",
    "    bag = bag_of_words(pattern_sentence, all_words)\n",
    "    X_train.append(bag)\n",
    "    \n",
    "    label = tags.index(tag)\n",
    "    y_train.append(label)\n",
    "    \n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be475c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatDataset(Dataset):\n",
    "    def __init__(self, X_train, y_train):\n",
    "        self.n_samples = len(X_train)\n",
    "        self.x_data = torch.from_numpy(X_train)\n",
    "        self.y_data = torch.from_numpy(y_train).type(torch.LongTensor)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return self.x_data[idx], self.y_data[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "    \n",
    "batch_size = 2048\n",
    "    \n",
    "dataset = ChatDataset(X_train, y_train)\n",
    "train_loader = DataLoader(dataset=dataset, batch_size=batch_size, num_workers=0, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce06dfe5",
   "metadata": {},
   "source": [
    "<a name = \"NN\"></a>\n",
    "## 5. Define Neural Net\n",
    "Define neural network for intent inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b789646",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.l1 = nn.Linear(input_size, hidden_size)\n",
    "        self.l2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.l3 = nn.Linear(hidden_size, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.l1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.l2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.l3(out)\n",
    "        return out\n",
    "\n",
    "input_size = len(X_train[0])\n",
    "hidden_size = 8\n",
    "output_size = len(tags)\n",
    "    \n",
    "model = NeuralNet(input_size, hidden_size, output_size).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7587b6",
   "metadata": {},
   "source": [
    "<a name = \"Train\"></a>\n",
    "## 6. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "08dbf373",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/500, Loss = 2.9721\n",
      "Epoch 100/500, Loss = 2.0750\n",
      "Epoch 150/500, Loss = 1.2561\n",
      "Epoch 200/500, Loss = 0.7099\n",
      "Epoch 250/500, Loss = 0.4482\n",
      "Epoch 300/500, Loss = 0.3302\n",
      "Epoch 350/500, Loss = 0.2997\n",
      "Epoch 400/500, Loss = 0.2271\n",
      "Epoch 450/500, Loss = 0.1784\n",
      "Epoch 500/500, Loss = 0.1427\n",
      "Final loss = 0.1427\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.001\n",
    "num_epochs = 500\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for words, labels in train_loader:\n",
    "        words, labels = words.to(device), labels.to(device)\n",
    "        \n",
    "        # forward\n",
    "        outputs = model(words)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    if (epoch+1)%50 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss = {loss.item():.4f}\")\n",
    "    \n",
    "print(f\"Final loss = {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a56e8577",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'model_state': model.state_dict(),\n",
    "    'input_size': input_size,\n",
    "    'output_size': output_size,\n",
    "    'hidden_size': hidden_size,\n",
    "    'all_words': all_words,\n",
    "    'tags': tags\n",
    "}\n",
    "\n",
    "torch.save(data, \"model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e3b8e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
